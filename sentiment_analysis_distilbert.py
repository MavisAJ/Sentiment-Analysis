# -*- coding: utf-8 -*-
"""Sentiment Analysis Distilbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_pBeGfELFvWCtBnJvUEK9e4lvS9Enco
"""

!pip install datasets transformers huggingface_hub
!apt-get install git-lfs

import pandas as pd
import numpy as np
#from datasets import load_metric
import os
import matplotlib.pyplot as plt
import wordcloud
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from sklearn.model_selection import train_test_split
from datasets import load_dataset, load_metric
import io
from transformers import AutoTokenizer, TrainingArguments, Trainer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
from google.colab import files
from google.colab import drive
from transformers import AutoModelForSequenceClassification
#converting training data to PyTorch tensors to speed up training and adding padding:
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
from huggingface_hub import notebook_login

# Disabe W&B
os.environ["WANDB_DISABLED"] = "true"

drive.mount('/content/drive')

train =pd.read_csv("/content/drive/MyDrive/PostBAP_ASSESSMENT/hugging.csv").dropna(axis = 0)
test = pd.read_csv("/content/drive/MyDrive/PostBAP_ASSESSMENT/Testhugging.csv").fillna("")

"""EDA"""

train.head()

test.head()

train['label'].unique

train.isnull().sum()

train, eval = train_test_split(train, test_size=0.2, random_state=42, stratify= train['label'])

print(f"new dataframe shapes: train is {train.shape}, eval is {eval.shape}")

# Save splitted subsets
train.to_csv("/content/drive/MyDrive/PostBAP_ASSESSMENT/train_subset.csv", index=False)
eval.to_csv("/content/drive/MyDrive/PostBAP_ASSESSMENT/eval_subset.csv", index=False)

dataset = load_dataset('csv',
                        data_files={'train': '/content/drive/MyDrive/PostBAP_ASSESSMENT/train_subset.csv',
                        'eval': '/content/drive/MyDrive/PostBAP_ASSESSMENT/eval_subset.csv'}, encoding = "ISO-8859-1")

def transform_labels(label):

    label = label['label']
    num = 0
    if label == -1: #'Negative'
        num = 0
    elif label == 0: #'Neutral'
        num = 1
    elif label == 1: #'Positive'
        num = 2

    return {'labels': num}

def tokenize_data(example):
    return tokenizer(example['safe_text'], padding='max_length')

# Change the tweets to tokens that the models can exploit
dataset = dataset.map(tokenize_data, batched=True)

# Transform	labels and remove the useless columns
remove_columns = ['tweet_id', 'label', 'safe_text', 'agreement']
dataset = dataset.map(transform_labels, remove_columns=remove_columns)

dataset

training_args = TrainingArguments("test_trainer", num_train_epochs= 3, load_best_model_at_end=True,evaluation_strategy= "steps")

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=3)

train_dataset = dataset['train'].shuffle(seed=20) #.select(range(40000)) # to select a part
eval_dataset = dataset['eval'].shuffle(seed=20)

trainer = Trainer(
    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset
)

trainer.train()

#defining the evaluation metrics
metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, 
                  eval_dataset=eval_dataset,compute_metrics=compute_metrics)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Launch the final evaluation 
trainer.evaluate()

notebook_login()

#saving model to hub
model.push_to_hub("MavisAJ/Sentiment_Analysis")

#saving tokenizer to hub
tokenizer.push_to_hub("MavisAJ/Sentiment_Analysis")